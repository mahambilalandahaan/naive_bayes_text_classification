{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Day 1 – IMDb Sentiment Analysis\n",
        "\n",
        "Steps to follow:\n",
        "\n",
        "Load Dataset\n",
        "\n",
        "Use datasets library or download from IMDb manually.\n",
        "\n",
        "Inspect the data: what are the columns? How many samples?\n",
        "\n",
        "Preprocess Text\n",
        "\n",
        "Remove punctuation, lowercase text.\n",
        "\n",
        "Optionally remove stopwords.\n",
        "\n",
        "Check for empty or missing reviews.\n",
        "\n",
        "Encode Labels\n",
        "\n",
        "IMDb labels are usually 0 (negative) and 1 (positive).\n",
        "\n",
        "Make sure they are numeric if needed.\n",
        "\n",
        "Train/Test Split\n",
        "\n",
        "Use train_test_split from scikit-learn.\n",
        "\n",
        "Example: 80% train, 20% test.\n",
        "\n",
        "Vectorize Text\n",
        "\n",
        "Use CountVectorizer/TfidfVectorizer from scikit-learn.\n",
        "\n",
        "Fit on training data, transform both train and test sets.\n",
        "\n",
        "Train Model\n",
        "\n",
        "Use MultinomialNB (Naive Bayes for text).\n",
        "\n",
        "Fit the model on training vectors and labels.\n",
        "\n",
        "Evaluate\n",
        "\n",
        "Predict on test data.\n",
        "\n",
        "Calculate accuracy and print a classification report.\n",
        "\n",
        "Reflection\n",
        "\n",
        "Compare performance.\n",
        "\n",
        "Check which words are most informative using .feature_log_prob_."
      ],
      "metadata": {
        "id": "VwSosdTxjghc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oz07buHTKj7X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3124315d-deb1-48e4-8ec9-109e764d2b1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    unsupervised: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n",
            "dict_keys(['train', 'test', 'unsupervised'])\n",
            "Train DataFrame head:\n",
            "                                                text  label\n",
            "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
            "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
            "2  If only to avoid making this type of film in t...      0\n",
            "3  This film was probably inspired by Godard's Ma...      0\n",
            "4  Oh, brother...after hearing about this ridicul...      0\n",
            "\n",
            "Test DataFrame head:\n",
            "                                                text  label\n",
            "0  I love sci-fi and am willing to put up with a ...      0\n",
            "1  Worth the entertainment value of a rental, esp...      0\n",
            "2  its a totally average film with a few semi-alr...      0\n",
            "3  STAR RATING: ***** Saturday Night **** Friday ...      0\n",
            "4  First off let me say, If you haven't enjoyed a...      0\n",
            "\n",
            "Train label distribution:\n",
            "label\n",
            "0    12500\n",
            "1    12500\n",
            "Name: count, dtype: int64\n",
            "(25000, 5000) (25000, 5000)\n",
            "Accuracy: 0.84088\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.83      0.85      0.84     12500\n",
            "    Positive       0.85      0.83      0.84     12500\n",
            "\n",
            "    accuracy                           0.84     25000\n",
            "   macro avg       0.84      0.84      0.84     25000\n",
            "weighted avg       0.84      0.84      0.84     25000\n",
            "\n",
            "\n",
            " Confusion matrix :\n",
            " [[10660  1840]\n",
            " [ 2138 10362]]\n",
            "Top 10 words for each class:\n",
            "Negative influential words:\n",
            "br movie film like just good bad really time dont\n",
            "Positive influential words:\n",
            "br film movie like good just story great time really\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['imdb_vectorizer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "#install dataset library if not done yet\n",
        "!pip install -q scikit_learn pandas\n",
        "#import\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "#Load IMDb dataset\n",
        "imdb=load_dataset(\"imdb\")\n",
        "#check keys to know about data format\n",
        "print(imdb)\n",
        "print(imdb.keys())\n",
        "#Convert train and test splits to pandas DataFrames\n",
        "train_df=pd.DataFrame({\n",
        "    \"text\": imdb['train']['text'],\n",
        "    \"label\": imdb['train']['label']\n",
        "})\n",
        "test_df=pd.DataFrame({\n",
        "    \"text\": imdb['test']['text'],\n",
        "    \"label\": imdb['test']['label']\n",
        "})\n",
        "\n",
        "print(\"Train DataFrame head:\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"\\nTest DataFrame head:\")\n",
        "print(test_df.head())\n",
        "\n",
        "#check label distribution in train set\n",
        "print(\"\\nTrain label distribution:\")\n",
        "print(train_df['label'].value_counts())\n",
        "#standardizing data\n",
        "import string\n",
        "def standardized_text(text):\n",
        "   return text.str.lower().str.replace(r'[^\\w\\s]','',regex = True)\n",
        "\n",
        "train_df['text']=standardized_text(train_df['text'])\n",
        "test_df['text']=standardized_text(test_df['text'])\n",
        "#apply countvectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# you can squeeze a bit more performance by adding stop-word removal or limiting features\n",
        "vectorizer=CountVectorizer(stop_words='english',max_features=5000,ngram_range=(1,2))\n",
        "X_train_counts=vectorizer.fit_transform(train_df['text'])\n",
        "X_test_counts=vectorizer.transform(test_df['text'])\n",
        "print(X_train_counts.shape,X_test_counts.shape)\n",
        "#Train a classifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
        "clf=MultinomialNB()\n",
        "clf.fit(X_train_counts,train_df['label'])\n",
        "#prediction\n",
        "label_pred=clf.predict(X_test_counts)\n",
        "#Evaluation\n",
        "print(\"Accuracy:\",accuracy_score(test_df['label'],label_pred))\n",
        "print(\"\\nClassification Report:\\n\",classification_report(test_df['label'],label_pred,target_names=['Negative','Positive']))\n",
        "print(\"\\n Confusion matrix :\\n\",confusion_matrix(test_df['label'],label_pred))\n",
        "#\n",
        "#If you want to see which words are most influential:\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "log_probs = clf.feature_log_prob_\n",
        "print(\"Top 10 words for each class:\")\n",
        "print(\"Negative influential words:\")\n",
        "neg_words=\" \".join([feature_names[i] for i in log_probs[0].argsort()[-10:][::-1]])\n",
        "print(neg_words)\n",
        "print(\"Positive influential words:\")\n",
        "pos_words=\" \".join([feature_names[i] for i in log_probs[1].argsort()[-10:][::-1]])\n",
        "print(pos_words)\n",
        "\n",
        "\n",
        "# working on joblib to save codes\n",
        "import joblib\n",
        "joblib.dump(clf, \"imdb_nb_model.pkl\")\n",
        "joblib.dump(vectorizer, \"imdb_vectorizer.pkl\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1️⃣ Load & Inspect Data\n",
        "from datasets import load_dataset\n",
        "imdb = load_dataset(\"imdb\")\n",
        "\n",
        "\n",
        "Downloads the IMDb dataset (25 k training + 25 k test movie reviews).\n",
        "\n",
        "imdb['train'] and imdb['test'] are the two splits.\n",
        "\n",
        "train_df = pd.DataFrame({\"text\": imdb['train']['text'],\n",
        "                         \"label\": imdb['train']['label']})\n",
        "\n",
        "\n",
        "Converts the Hugging Face dataset into regular pandas DataFrames for easier handling.\n",
        "\n",
        "2️⃣ Text Cleaning\n",
        "def standardized_text(text):\n",
        "    return text.str.lower().str.replace(r'[^\\w\\s]','',regex=True)\n",
        "\n",
        "\n",
        "Lowercases everything.\n",
        "\n",
        "Removes punctuation and other non-word characters.\n",
        "\n",
        "3️⃣ Feature Extraction\n",
        "vectorizer = CountVectorizer(stop_words='english',\n",
        "                             max_features=5000,\n",
        "                             ngram_range=(1,2))\n",
        "\n",
        "\n",
        "Turns each review into a bag-of-words matrix of counts.\n",
        "\n",
        "Uses English stop-word removal.\n",
        "\n",
        "Keeps the 5 000 most frequent single words and two-word phrases (bigrams).\n",
        "\n",
        "4️⃣ Train the Model\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_counts, train_df['label'])\n",
        "\n",
        "\n",
        "Fits a Multinomial Naive Bayes classifier on the training vectors.\n",
        "\n",
        "5️⃣ Evaluate\n",
        "\n",
        "accuracy_score, classification_report, and confusion_matrix show performance on the test set.\n",
        "\n",
        "The section with feature_log_prob_ lists the top 10 words most indicative of negative vs positive sentiment.\n",
        "\n",
        "6️⃣ Save for Later\n",
        "import joblib\n",
        "joblib.dump(clf, \"imdb_nb_model.pkl\")\n",
        "joblib.dump(vectorizer, \"imdb_vectorizer.pkl\")\n",
        "\n",
        "\n",
        "joblib.dump writes any Python object to disk in a fast, compressed format:\n",
        "\n",
        "imdb_nb_model.pkl → your trained Naive Bayes model.\n",
        "\n",
        "imdb_vectorizer.pkl → the fitted CountVectorizer.\n",
        "\n",
        "Later you can load them back without retraining:\n",
        "\n",
        "from joblib import load\n",
        "clf = load(\"imdb_nb_model.pkl\")\n",
        "vectorizer = load(\"imdb_vectorizer.pkl\")\n",
        "\n",
        "# Predict a new review\n",
        "new_text = [\"The movie was absolutely fantastic!\"]\n",
        "X_new = vectorizer.transform(new_text)\n",
        "print(clf.predict(X_new))   # → 1 for positive, 0 for negative\n",
        "\n",
        "\n",
        "This way, you can deploy the classifier or run predictions in a new script or on a server without repeating the entire training pipeline."
      ],
      "metadata": {
        "id": "-mKm_bCke9di"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X1asY6P0e_aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpHiKXNJfGAQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a08ea0ce-e6d5-490a-d991-fc03051b7dd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    unsupervised: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n",
            "dict_keys(['train', 'test', 'unsupervised'])\n",
            "Train DataFrame head:\n",
            "                                                text  label\n",
            "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
            "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
            "2  If only to avoid making this type of film in t...      0\n",
            "3  This film was probably inspired by Godard's Ma...      0\n",
            "4  Oh, brother...after hearing about this ridicul...      0\n",
            "\n",
            "Test DataFrame head:\n",
            "                                                text  label\n",
            "0  I love sci-fi and am willing to put up with a ...      0\n",
            "1  Worth the entertainment value of a rental, esp...      0\n",
            "2  its a totally average film with a few semi-alr...      0\n",
            "3  STAR RATING: ***** Saturday Night **** Friday ...      0\n",
            "4  First off let me say, If you haven't enjoyed a...      0\n",
            "\n",
            "Train label distribution:\n",
            "label\n",
            "0    12500\n",
            "1    12500\n",
            "Name: count, dtype: int64\n",
            "(25000, 5000) (25000, 5000)\n",
            "Accuracy: 0.84928\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.85      0.85      0.85     12500\n",
            "    Positive       0.85      0.85      0.85     12500\n",
            "\n",
            "    accuracy                           0.85     25000\n",
            "   macro avg       0.85      0.85      0.85     25000\n",
            "weighted avg       0.85      0.85      0.85     25000\n",
            "\n",
            "\n",
            " Confusion matrix :\n",
            " [[10642  1858]\n",
            " [ 1910 10590]]\n",
            "Top 10 words for each class:\n",
            "Negative influential words:\n",
            "br movie film just like bad good really dont time\n",
            "Positive influential words:\n",
            "br film movie great good like story just time really\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['imdb_vectorizer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "\n",
        "#import\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "#Load IMDb dataset\n",
        "imdb=load_dataset(\"imdb\")\n",
        "#check keys to know about data format\n",
        "print(imdb)\n",
        "print(imdb.keys())\n",
        "#Convert train and test splits to pandas DataFrames\n",
        "train_df=pd.DataFrame({\n",
        "    \"text\": imdb['train']['text'],\n",
        "    \"label\": imdb['train']['label']\n",
        "})\n",
        "test_df=pd.DataFrame({\n",
        "    \"text\": imdb['test']['text'],\n",
        "    \"label\": imdb['test']['label']\n",
        "})\n",
        "\n",
        "print(\"Train DataFrame head:\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"\\nTest DataFrame head:\")\n",
        "print(test_df.head())\n",
        "\n",
        "#check label distribution in train set\n",
        "print(\"\\nTrain label distribution:\")\n",
        "print(train_df['label'].value_counts())\n",
        "#standardizing data\n",
        "import string\n",
        "def standardized_text(text):\n",
        "   return text.str.lower().str.replace(r'[^\\w\\s]','',regex = True)\n",
        "\n",
        "train_df['text']=standardized_text(train_df['text'])\n",
        "test_df['text']=standardized_text(test_df['text'])\n",
        "#apply countvectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# you can squeeze a bit more performance by adding stop-word removal or limiting features\n",
        "vectorizer=TfidfVectorizer(stop_words='english',max_features=5000,ngram_range=(1,2))\n",
        "X_train_tfidf=vectorizer.fit_transform(train_df['text'])\n",
        "X_test_tfidf=vectorizer.transform(test_df['text'])\n",
        "print(X_train_tfidf.shape,X_test_tfidf.shape)\n",
        "#Train a classifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
        "clf1=MultinomialNB()\n",
        "clf1.fit(X_train_tfidf,train_df['label'])\n",
        "#prediction\n",
        "label_pred=clf1.predict(X_test_tfidf)\n",
        "#Evaluation\n",
        "print(\"Accuracy:\",accuracy_score(test_df['label'],label_pred))\n",
        "print(\"\\nClassification Report:\\n\",classification_report(test_df['label'],label_pred,target_names=['Negative','Positive']))\n",
        "print(\"\\n Confusion matrix :\\n\",confusion_matrix(test_df['label'],label_pred))\n",
        "#\n",
        "#If you want to see which words are most influential:\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "log_probs = clf1.feature_log_prob_\n",
        "print(\"Top 10 words for each class:\")\n",
        "print(\"Negative influential words:\")\n",
        "neg_words=\" \".join([feature_names[i] for i in log_probs[0].argsort()[-10:][::-1]])\n",
        "print(neg_words)\n",
        "print(\"Positive influential words:\")\n",
        "pos_words=\" \".join([feature_names[i] for i in log_probs[1].argsort()[-10:][::-1]])\n",
        "print(pos_words)\n",
        "\n",
        "\n",
        "# working on joblib to save codes\n",
        "import joblib\n",
        "joblib.dump(clf1, \"imdb_nb_tfidf_model.pkl\")\n",
        "joblib.dump(vectorizer, \"imdb_tfidf_vectorizer.pkl\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pIxz8wQYh2xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1️⃣ Load & Inspect Data\n",
        "from datasets import load_dataset\n",
        "imdb = load_dataset(\"imdb\")\n",
        "\n",
        "\n",
        "Downloads the IMDb dataset (25 k training + 25 k test movie reviews).\n",
        "\n",
        "imdb['train'] and imdb['test'] are the two splits.\n",
        "\n",
        "train_df = pd.DataFrame({\"text\": imdb['train']['text'],\n",
        "                         \"label\": imdb['train']['label']})\n",
        "\n",
        "\n",
        "Converts the Hugging Face dataset into regular pandas DataFrames for easier handling.\n",
        "\n",
        "2️⃣ Text Cleaning\n",
        "def standardized_text(text):\n",
        "    return text.str.lower().str.replace(r'[^\\w\\s]','',regex=True)\n",
        "\n",
        "\n",
        "Lowercases everything.\n",
        "\n",
        "Removes punctuation and other non-word characters.\n",
        "\n",
        "3️⃣ Feature Extraction\n",
        "vectorizer = TfidfVectorizer(stop_words='english',\n",
        "                             max_features=5000,\n",
        "                             ngram_range=(1,2))\n",
        "\n",
        "\n",
        "Turns each review into a bag-of-words matrix of counts.\n",
        "\n",
        "Uses English stop-word removal.\n",
        "\n",
        "Keeps the 5 000 most frequent single words and two-word phrases (bigrams).\n",
        "\n",
        "4️⃣ Train the Model\n",
        "clf1 = MultinomialNB()\n",
        "clf1.fit(X_train_tfidf, train_df['label'])\n",
        "\n",
        "\n",
        "Fits a Multinomial Naive Bayes classifier on the training vectors.\n",
        "\n",
        "5️⃣ Evaluate\n",
        "\n",
        "accuracy_score, classification_report, and confusion_matrix show performance on the test set.\n",
        "\n",
        "The section with feature_log_prob_ lists the top 10 words most indicative of negative vs positive sentiment.\n",
        "\n",
        "6️⃣ Save for Later\n",
        "import joblib\n",
        "joblib.dump(clf1, \"imdb_nb_tfidf_model.pkl\")\n",
        "joblib.dump(vectorizer, \"imdb_tfidf_vectorizer.pkl\")\n",
        "\n",
        "\n",
        "joblib.dump writes any Python object to disk in a fast, compressed format:\n",
        "\n",
        "imdb_nb_model.pkl → your trained Naive Bayes model.\n",
        "\n",
        "imdb_vectorizer.pkl → the fitted TfidfVectorizer.\n",
        "\n",
        "Later you can load them back without retraining:\n",
        "\n",
        "from joblib import load\n",
        "clf = load(\"imdb_nb_tfidf_model.pkl\")\n",
        "vectorizer = load(\"imdb_tfidf_vectorizer.pkl\")\n",
        "\n",
        "# Predict a new review\n",
        "new_text = [\"The movie was absolutely fantastic!\"]\n",
        "X_new = vectorizer.transform(new_text)\n",
        "print(clf1.predict(X_new))   # → 1 for positive, 0 for negative\n",
        "\n",
        "\n",
        "This way, you can deploy the classifier or run predictions in a new script or on a server without repeating the entire training pipeline."
      ],
      "metadata": {
        "id": "fzK8qVTeh3jR"
      }
    }
  ]
}